# Duck Hunt - Claude Code Project Guide

A DuckDB extension for parsing test results, build outputs, and CI/CD logs from 90+ development tools into a standardized 39-field schema.

## Quick Reference

```bash
# Build
make release           # Full release build
make debug             # Debug build

# Test
make test              # Run all tests
./build/release/test/unittest "test/sql/*.test"  # Run specific tests

# Quick verification
./build/release/duckdb -s "LOAD duck_hunt; SELECT * FROM duck_hunt_formats() LIMIT 5;"
```

## Using blq for Build/Test

Prefer using the `blq` MCP server for running builds and tests. It captures output, parses errors, and enables querying results.

**Registered commands** (already configured):
- `build` - `make -j 4` (release build)
- `build-debug` - `make debug -j 4`
- `test` - `make test`

```python
# Run commands
mcp__blq__run(command="build")
mcp__blq__run(command="test")

# Run with extra arguments
mcp__blq__run(command="test", extra=["TESTTARGET=test/sql/my_feature.test"])

# Query results
mcp__blq__errors()              # Recent errors
mcp__blq__status()              # Build status summary
mcp__blq__history()             # Run history
mcp__blq__context(ref="1:5")    # Context around error
mcp__blq__diff(run1=1, run2=2)  # Compare errors between runs

# Register new commands as needed
mcp__blq__register_command(name="unittest", cmd="./build/release/test/unittest")
```

Benefits:
- Errors are parsed and indexed for querying
- Build history is preserved across runs
- Can compare error counts between runs
- Output is captured even for long-running builds

## Project Structure

```
duck_hunt/
├── src/
│   ├── parsers/              # 17 parser categories, 95 formats
│   │   ├── base/             # BaseParser class, registry
│   │   ├── test_frameworks/  # pytest, junit, gtest, rspec, etc.
│   │   ├── linting_tools/    # eslint, pylint, mypy, clippy, etc.
│   │   ├── build_systems/    # make, cmake, cargo, maven, etc.
│   │   ├── workflow_engines/ # github_actions, gitlab_ci, jenkins
│   │   ├── app_logging/      # winston, pino, log4j, serilog
│   │   └── ...               # cloud_logs, infrastructure, etc.
│   ├── core/                 # Event schema, utilities
│   ├── include/              # Header files
│   └── *.cpp                 # Table/scalar functions
├── test/
│   ├── sql/                  # DuckDB test files (.test)
│   └── samples/              # Sample log files by category
├── docs/                     # Documentation
│   ├── schema.md             # 39-field schema reference
│   ├── formats.md            # All format strings
│   ├── field_mappings.md     # Domain-specific mappings
│   └── migration-guide.md    # Schema v2 breaking changes
└── duckdb/                   # DuckDB submodule
```

## Core Concepts

### Main Functions

```sql
-- Parse log files (supports globs, compression)
read_duck_hunt_log(source, format, severity_threshold, ignore_errors, content, context)

-- Parse inline content
parse_duck_hunt_log(text, format, severity_threshold, content, context)

-- Workflow parsing with hierarchy (job/step structure)
read_duck_hunt_workflow_log(source, format, severity_threshold, ignore_errors)

-- Utilities
duck_hunt_formats()           -- List all formats
duck_hunt_detect_format()     -- Auto-detect format
duck_hunt_diagnose_read()     -- Debug format detection
status_badge()                -- Generate [PASS]/[FAIL] badges
```

### Output Schema (39 fields)

Key field groups:
- **Core:** `event_id`, `tool_name`, `event_type`
- **Reference Location:** `ref_file`, `ref_line`, `ref_column` (source code locations)
- **Log Tracking:** `log_file`, `log_line_start`, `log_line_end` (position in log)
- **Classification:** `status`, `severity`, `category`, `error_code`
- **Content:** `message`, `suggestion`, `log_content`, `structured_data`
- **Hierarchy:** `scope`→`group`→`unit`→`subunit` (generic 4-level)
- **Pattern Analysis:** `fingerprint`, `pattern_id`, `similarity_score`

Note: `group` is a SQL reserved word - use `"group"` (quoted) in queries.

### Format Groups

Instead of exact formats, use groups for auto-detection:
- **Language:** `python`, `java`, `c_cpp`, `javascript`, `rust`, `go`, `ruby`, `dotnet`
- **Tool-type:** `lint`, `test`, `build`, `security`, `infrastructure`, `logging`, `ci`

## Development Guidelines

### Adding a New Parser

1. Create `src/parsers/<category>/<name>_parser.cpp`
2. Inherit from `BaseParser` (see existing parsers for patterns)
3. Implement `CanParse()` and `Parse()` methods
4. Register in `src/parsers/<category>/init.cpp`
5. Add tests in `test/sql/<category>.test` or `new_formats.test`
6. Add sample file in `test/samples/<category>/`
7. Update docs: `PARSERS.md`, `docs/formats.md`

### Parser Implementation Pattern

```cpp
class MyToolParser : public BaseParser {
public:
    MyToolParser() : BaseParser("mytool", 50) {}  // name, priority

    bool CanParse(const string& content) override {
        // Quick detection - check for distinctive patterns
        return content.find("MYTOOL:") != string::npos;
    }

    vector<ValidationEvent> Parse(const string& content) override {
        vector<ValidationEvent> events;
        // Parse content, create ValidationEvent for each error/warning
        return events;
    }
};
```

### Testing

Tests use DuckDB's `.test` format:

```sql
# test/sql/my_feature.test

statement ok
LOAD duck_hunt;

query III
SELECT ref_file, ref_line, status
FROM parse_duck_hunt_log('error at file.c:10', 'mytool')
----
file.c	10	ERROR
```

Run specific tests:
```bash
./build/release/test/unittest "test/sql/my_feature.test"
```

### Key Conventions

- **ref_file/ref_line** = Source code location referenced in log
- **log_file/log_line_start** = Position within the log itself
- **status** = Semantic outcome (PASS, FAIL, ERROR, WARNING, INFO, SKIP)
- **severity** = Importance level (debug, info, warning, error, critical)
- **fingerprint** = Normalized message hash for clustering similar errors

### Hierarchy Mapping

The generic hierarchy adapts to each domain:

| Level | CI/CD | Kubernetes | Tests | App Logs |
|-------|-------|------------|-------|----------|
| scope | Workflow | Cluster | Suite | Service |
| group | Job | Namespace | Class | Component |
| unit | Step | Pod | Method | Handler |
| subunit | - | Container | - | - |

## Common Tasks

### Debugging Format Detection

```sql
-- See which parsers match content
SELECT format, can_parse, events_produced, is_selected
FROM duck_hunt_diagnose_read('mystery.log');

-- Or for inline content
FROM duck_hunt_diagnose_parse('...log content...');
```

### Testing Content Parameter

The `content` parameter controls `log_content` field size:
- `content := 'full'` - Full content (default)
- `content := 200` - Limit to 200 chars
- `content := 'smart'` - Intelligent truncation
- `content := 'none'` - Omit entirely (NULL)

### Context Extraction

Include surrounding log lines for each event:
```sql
SELECT message, context
FROM read_duck_hunt_log('build.log', context := 3);
-- context is LIST(STRUCT(line_number, content, is_event))
```

## Build Notes

- Uses DuckDB extension template (`extension-ci-tools/`)
- Requires C++17
- Optional: `webbed` extension for XML parsing (junit_xml, cobertura_xml)
- Compression support: `.gz` built-in, `.zst` requires parquet extension

### Updating DuckDB Version

See `docs/UPDATING.md`:
1. Update `duckdb/` submodule to latest tag
2. Update `extension-ci-tools/` to matching branch
3. Update workflow versions in `.github/workflows/`

## Resources

- [README.md](README.md) - User-facing documentation
- [PARSERS.md](PARSERS.md) - Parser list with format names
- [AGENTS.md](AGENTS.md) - AI agent usage guide
- [docs/schema.md](docs/schema.md) - Complete field documentation
- [docs/formats.md](docs/formats.md) - All 90+ format strings
- [docs/field_mappings.md](docs/field_mappings.md) - Domain-specific mappings
- [docs/migration-guide.md](docs/migration-guide.md) - Schema v2 changes
