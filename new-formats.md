### Draft Specification: Suggested Formats for Inclusion in Duck Hunt v1.1.0

#### Overview
The following proposal outlines additional formats for inclusion in Duck Hunt v1.1.0. These formats focus on expanding support for logs and output relevant to **DevOps, security monitoring**, and infrastructure management use cases in addition to the current development-focused toolset. Each proposed addition addresses real-world needs in monitoring, compliance, and operational visibility.

---

### Proposed New Formats

#### **1. Apache HTTP Server Access Logs**
- **Use Case**: Monitoring web traffic, detecting anomalies, and analyzing HTTP request patterns.
- **Sample Format**:
  ```
  127.0.0.1 - - [12/Dec/2025:10:15:42 +0000] "GET /index.html HTTP/1.1" 200 1024
  ```
- **Proposed Format String**: `apache_access`
- **Use in Duck Hunt**:
  ```sql
  SELECT ip_address, request_method, status_code, response_size
  FROM read_duck_hunt_log('apache_access.log', 'apache_access')
  WHERE status_code != '200';
  ```

#### **2. NGINX Access Logs**
- **Use Case**: Similar to Apache logs, for monitoring traffic and load balancing analytics.
- **Sample Format**:
  ```
  192.168.1.10 - - [12/Dec/2025:12:34:56 +0000] "POST /api/v1/login HTTP/2.0" 401 512
  ```
- **Proposed Format String**: `nginx_access`
- **Use in Duck Hunt**:
  ```sql
  SELECT ip_address, request_path, status_code
  FROM read_duck_hunt_log('nginx_access.log', 'nginx_access')
  WHERE status_code >= '400';
  ```

#### **3. Syslog**
- **Use Case**: Capturing and analyzing system logs generated by various applications and services.
- **Sample Format**:
  ```
  Dec 12 10:15:42 localhost sshd[1234]: Accepted password for user from 10.0.0.1 port 22
  ```
- **Proposed Format String**: `syslog`
- **Use in Duck Hunt**:
  ```sql
  SELECT timestamp, service_name, message
  FROM read_duck_hunt_log('syslog', 'syslog')
  WHERE service_name = 'sshd';
  ```

#### **4. AWS CloudTrail Logs**
- **Use Case**: Monitoring AWS API calls for compliance and security event tracking.
- **Sample Format** (JSON):
  ```json
  {
    "eventTime": "2025-12-12T11:20:42Z",
    "eventName": "CreateUser",
    "awsRegion": "us-east-1",
    "sourceIPAddress": "192.168.1.100"
  }
  ```
- **Proposed Format String**: `aws_cloudtrail`
- **Use in Duck Hunt**:
  ```sql
  SELECT event_time, event_name, aws_region, source_ip_address
  FROM read_duck_hunt_log('cloudtrail.json', 'aws_cloudtrail')
  WHERE event_name = 'CreateUser';
  ```

#### **5. Kubernetes Application Logs**
- **Use Case**: Debugging and monitoring workloads running in Kubernetes clusters.
- **Sample Format**:
  ```
  {"level":"error","ts":"2025-12-12T15:30:00.123Z","msg":"Pod failed to start","namespace":"production","pod":"web-server"}
  ```
- **Proposed Format String**: `k8s_logs`
- **Use in Duck Hunt**:
  ```sql
  SELECT timestamp, severity, message, namespace, pod_name
  FROM read_duck_hunt_log('kubernetes.log', 'k8s_logs')
  WHERE severity = 'error';
  ```

#### **6. JSON Audit Logs**
- **Use Case**: Capturing audit trails for compliance across apps and infrastructures.
- **Sample Format (Generic JSON)**:
  ```json
  {
    "event_type": "LOGIN_ATTEMPT",
    "timestamp": "2025-12-12T18:45:00Z",
    "user": "admin",
    "status": "FAILURE"
  }
  ```
- **Proposed Format String**: `json_audit`
- **Use in Duck Hunt**:
  ```sql
  SELECT event_type, user, timestamp
  FROM read_duck_hunt_log('audit.json', 'json_audit')
  WHERE status = 'FAILURE';
  ```

#### **7. Firewall Logs (e.g., IPTables)**
- **Use Case**: Tracking network traffic and blocking unauthorized connections.
- **Sample Format**:
  ```
  Dec 12 10:15:42 firewall kernel: IN=eth0 OUT= MAC=00:00:5e:00:53:11 SRC=192.168.1.5 DST=192.168.1.1 LEN=60 TOS=0x00
  ```
- **Proposed Format String**: `firewall_logs`
- **Use in Duck Hunt**:
  ```sql
  SELECT timestamp, source_ip, destination_ip, message
  FROM read_duck_hunt_log('firewall.log', 'firewall_logs')
  WHERE source_ip LIKE '192.%';
  ```

---

### Summary

**Why These Formats?**
The selected formats provide a strong basis for extending Duck Hunt's capabilities beyond development outputs, helping DevOps and security teams monitor, debug, and secure production and staging environments.

**Implementation Priority:**
1. **Syslog**
2. **HTTP Logs** (`apache_access`, `nginx_access`)
3. **CloudTrail Logs**
4. **Firewall Logs**
5. **Kubernetes Logs**
6. **JSON Audit Logs**

Would you like me to expand on any of these formats or provide mock code for their integration?

Here are additional potential formats, beyond the 7 proposed earlier, that could be valuable for v1.1.0 in **DevOps** and **security monitoring** use cases:

### **Additional Format Suggestions**

#### **8. Windows Event Logs**
- **Use Case**: Capturing security, system, and application events on Windows servers or desktops.
- **Sample Format** (Exported as text or JSON):
  ```
  {
    "EventID": 4625,
    "Timestamp": "2025-12-12T10:00:00Z",
    "LogLevel": "Error",
    "Message": "Failed logon attempt."
  }
  ```
- **Proposed Format String**: `windows_event`
- **Use in Duck Hunt**:
  ```sql
  SELECT event_id, timestamp, log_level, message
  FROM read_duck_hunt_log('windows_event.json', 'windows_event')
  WHERE log_level = 'Error';
  ```

#### **9. SSH Logs**
- **Use Case**: Monitoring login attempts, including failed password attempts, for security threats.
- **Sample Format**:
  ```
  Dec 12 10:15:43 hostname sshd[1234]: Failed password for invalid user admin from 192.168.1.5 port 22
  ```
- **Proposed Format String**: `ssh_logs`
- **Use in Duck Hunt**:
  ```sql
  SELECT timestamp, user, ip_address, status
  FROM read_duck_hunt_log('auth.log', 'ssh_logs')
  WHERE status = 'Failed';
  ```

#### **10. Auditd Logs**
- **Use Case**: Capturing Linux system audit events (e.g., file access, program execution, user actions).
- **Sample Format**:
  ```
  type=SYSCALL msg=audit(1670846542.536): arch=c000003e syscall=59 success=yes
  ```
- **Proposed Format String**: `auditd_logs`
- **Use in Duck Hunt**:
  ```sql
  SELECT type, message, success
  FROM read_duck_hunt_log('auditd.log', 'auditd_logs')
  WHERE success = 'no';
  ```

#### **11. Firewall (Web Application Firewall or Proxy Logs)**
- **Use Case**: Monitoring and analyzing HTTP-based security threats like SQL injection or XSS attacks.
- **Sample Format**:
  ```
  192.168.1.5 - - [12/Dec/2025:17:00:00 +0000] "GET /index.php?id=1' OR 1 -- HTTP/1.1" 403
  ```
- **Proposed Format String**: `waf_logs`
- **Use in Duck Hunt**:
  ```sql
  SELECT ip_address, request_path, status_code, attack_vector
  FROM read_duck_hunt_log('waf.log', 'waf_logs')
  WHERE status_code != '200';
  ```

#### **12. AWS VPC Flow Logs**
- **Use Case**: Analyzing network connections, performance, or suspicious packet activity in AWS.
- **Sample Format**:
  ```
  02:31:22 accept 192.168.1.10 10.10.5.5 443 12345
  ```
- **Proposed Format String**: `vpc_flow`
- **Use in Duck Hunt**:
  ```sql
  SELECT timestamp, accept, source_ip, destination_port
  FROM read_duck_hunt_log('vpc_flow.log', 'vpc_flow')
  WHERE accept = 'deny';
  ```

#### **13. S3 Access Logs**
- **Use Case**: Monitoring data access in AWS S3 buckets for anomalous or unexpected activity.
- **Sample Format**:
  ```
  192.168.1.1 [12/Dec/2025:12:15:42 +0000] GET /mybucket/file.txt 200
  ```
- **Proposed Format String**: `s3_access`
- **Use in Duck Hunt**:
  ```sql
  SELECT source_ip, request_path, status_code
  FROM read_duck_hunt_log('s3_access.log', 's3_access')
  WHERE status_code != '200';
  ```

#### **14. GCP Logs**
- **Use Case**: Logging GCP service events, usage, and security-related actions.
- **Sample Format**:
  ```json
  {
    "protoPayload": {
      "methodName": "google.logging.v2.WriteLogEntries",
      "status": {"code": 500},
      "authenticationInfo": {"principalEmail": "user@example.com"}
    }
  }
  ```
- **Proposed Format String**: `gcp_logs`
- **Use in Duck Hunt**:
  ```sql
  SELECT method_name, principal_email, status_code
  FROM read_duck_hunt_log('gcp.json', 'gcp_logs')
  WHERE status_code >= 500;
  ```

#### **15. Azure Activity Logs**
- **Use Case**: Auditing user operations across Azure services for compliance and anomaly detection.
- **Sample Format**:
  ```json
  {
    "time": "2025-12-12T12:34:56+00:00",
    "operationName": "Create User",
    "status": "Succeeded",
    "callerIpAddress": "192.168.1.1"
  }
  ```
- **Proposed Format String**: `azure_activity`
- **Use in Duck Hunt**:
  ```sql
  SELECT time, operation_name, caller_ip_address, status
  FROM read_duck_hunt_log('azure.json', 'azure_activity')
  WHERE status != 'Succeeded';
  ```

---

### How These Formats Extend Use Cases
These additional formats largely target **cloud environments, infrastructure, and security monitoring**, filling gaps in current tools:
1. Cloud provider logs (AWS, GCP, Azure) provide rich insights into user and service-based actions.
2. SSH and Auditd logs help with threat detection for compromised systems.
3. WAF logs allow monitoring for HTTP application-level threats.

---

### Application Logging Libraries

These formats cover application-level logging from various programming languages and frameworks, enabling duck_hunt to parse runtime logs alongside build/test output.

#### **16. Python Logging (stdlib)**
- **Use Case**: Parsing logs from Python's built-in logging module.
- **Sample Format**:
  ```
  2025-01-15 10:30:45,123 - myapp.module - INFO - User login successful
  2025-01-15 10:30:46,456 - myapp.module - ERROR - Database connection failed
  ```
- **Proposed Format String**: `python_logging`
- **Use in Duck Hunt**:
  ```sql
  SELECT timestamp, logger_name, severity, message
  FROM read_duck_hunt_log('app.log', 'python_logging')
  WHERE severity = 'ERROR';
  ```

#### **17. Structlog (Python)**
- **Use Case**: Structured logging for Python applications outputting JSON.
- **Sample Format**:
  ```json
  {"event": "user_login", "user_id": 123, "level": "info", "timestamp": "2025-01-15T10:30:45Z"}
  ```
- **Proposed Format String**: `structlog_json`
- **Use in Duck Hunt**:
  ```sql
  SELECT event, level, timestamp
  FROM read_duck_hunt_log('app.log', 'structlog_json')
  WHERE level = 'error';
  ```

#### **18. Loguru (Python)**
- **Use Case**: Popular Python logging alternative with distinctive colored output.
- **Sample Format**:
  ```
  2025-01-15 10:30:45.123 | INFO     | myapp.module:function:42 - Processing request
  2025-01-15 10:30:46.456 | ERROR    | myapp.module:function:55 - Request failed
  ```
- **Proposed Format String**: `loguru_text`
- **Use in Duck Hunt**:
  ```sql
  SELECT timestamp, severity, module, function_name, line_number, message
  FROM read_duck_hunt_log('app.log', 'loguru_text')
  WHERE severity = 'ERROR';
  ```

#### **19. Winston (Node.js)**
- **Use Case**: Most popular Node.js logging library.
- **Sample Format (JSON)**:
  ```json
  {"level":"error","message":"Connection timeout","service":"api","timestamp":"2025-01-15T10:30:45.123Z"}
  ```
- **Sample Format (Text)**:
  ```
  2025-01-15T10:30:45.123Z [api] error: Connection timeout
  ```
- **Proposed Format Strings**: `winston_json`, `winston_text`
- **Use in Duck Hunt**:
  ```sql
  SELECT level, service, message, timestamp
  FROM read_duck_hunt_log('app.log', 'winston_json')
  WHERE level = 'error';
  ```

#### **20. Pino (Node.js)**
- **Use Case**: Fast JSON logger for Node.js, commonly used in production.
- **Sample Format**:
  ```json
  {"level":30,"time":1705315845123,"pid":1234,"hostname":"server1","msg":"request completed"}
  {"level":50,"time":1705315846456,"pid":1234,"hostname":"server1","msg":"database error","err":{"message":"timeout"}}
  ```
- **Proposed Format String**: `pino_json`
- **Use in Duck Hunt**:
  ```sql
  SELECT level, hostname, msg, time
  FROM read_duck_hunt_log('app.log', 'pino_json')
  WHERE level >= 50;
  ```

#### **21. Bunyan (Node.js)**
- **Use Case**: JSON logging library for Node.js with structured output.
- **Sample Format**:
  ```json
  {"name":"myapp","hostname":"server1","pid":1234,"level":30,"msg":"listening on port 3000","time":"2025-01-15T10:30:45.123Z","v":0}
  ```
- **Proposed Format String**: `bunyan_json`
- **Use in Duck Hunt**:
  ```sql
  SELECT name, level, msg, time
  FROM read_duck_hunt_log('app.log', 'bunyan_json')
  WHERE level >= 40;
  ```

#### **22. Log4j / Logback (Java)**
- **Use Case**: Extremely common Java logging frameworks.
- **Sample Format**:
  ```
  2025-01-15 10:30:45,123 INFO  [main] com.example.App - Application started
  2025-01-15 10:30:46,456 ERROR [http-thread-1] com.example.Service - NullPointerException
  ```
- **Proposed Format String**: `log4j_text`
- **Use in Duck Hunt**:
  ```sql
  SELECT timestamp, severity, thread, class_name, message
  FROM read_duck_hunt_log('app.log', 'log4j_text')
  WHERE severity = 'ERROR';
  ```

#### **23. Java Stack Traces**
- **Use Case**: Parsing multiline Java exception stack traces.
- **Sample Format**:
  ```
  java.lang.NullPointerException: Cannot invoke method on null
      at com.example.Service.process(Service.java:42)
      at com.example.Controller.handle(Controller.java:15)
      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  Caused by: java.io.IOException: Connection reset
      at java.net.SocketInputStream.read(SocketInputStream.java:210)
  ```
- **Proposed Format String**: `java_stacktrace`
- **Use in Duck Hunt**:
  ```sql
  SELECT exception_type, message, file_path, line_number, caused_by
  FROM read_duck_hunt_log('error.log', 'java_stacktrace');
  ```

#### **24. Zap (Go)**
- **Use Case**: Uber's high-performance structured logger for Go.
- **Sample Format**:
  ```json
  {"level":"info","ts":1705315845.123,"caller":"server/main.go:42","msg":"server starting","port":8080}
  {"level":"error","ts":1705315846.456,"caller":"db/conn.go:55","msg":"connection failed","error":"timeout"}
  ```
- **Proposed Format String**: `zap_json`
- **Use in Duck Hunt**:
  ```sql
  SELECT level, caller, msg, ts
  FROM read_duck_hunt_log('app.log', 'zap_json')
  WHERE level = 'error';
  ```

#### **25. Logrus (Go)**
- **Use Case**: Popular structured logger for Go applications.
- **Sample Format (JSON)**:
  ```json
  {"level":"info","msg":"server started","port":8080,"time":"2025-01-15T10:30:45Z"}
  ```
- **Sample Format (Text)**:
  ```
  time="2025-01-15T10:30:45Z" level=info msg="server started" port=8080
  ```
- **Proposed Format Strings**: `logrus_json`, `logrus_text`
- **Use in Duck Hunt**:
  ```sql
  SELECT level, msg, time
  FROM read_duck_hunt_log('app.log', 'logrus_json')
  WHERE level = 'error';
  ```

#### **26. Zerolog (Go)**
- **Use Case**: Zero-allocation JSON logger for Go.
- **Sample Format**:
  ```json
  {"level":"info","time":"2025-01-15T10:30:45Z","message":"request processed","duration_ms":45}
  ```
- **Proposed Format String**: `zerolog_json`
- **Use in Duck Hunt**:
  ```sql
  SELECT level, message, time, duration_ms
  FROM read_duck_hunt_log('app.log', 'zerolog_json')
  WHERE level = 'error';
  ```

#### **27. Ruby Logger (stdlib)**
- **Use Case**: Ruby's built-in Logger class output.
- **Sample Format**:
  ```
  I, [2025-01-15T10:30:45.123456 #1234]  INFO -- myapp: User logged in
  E, [2025-01-15T10:30:46.456789 #1234] ERROR -- myapp: Connection failed
  ```
- **Proposed Format String**: `ruby_logger`
- **Use in Duck Hunt**:
  ```sql
  SELECT severity, timestamp, program, message
  FROM read_duck_hunt_log('app.log', 'ruby_logger')
  WHERE severity = 'ERROR';
  ```

#### **28. Rails Application Logs**
- **Use Case**: Ruby on Rails application and request logs.
- **Sample Format**:
  ```
  Started GET "/users" for 127.0.0.1 at 2025-01-15 10:30:45 +0000
  Processing by UsersController#index as HTML
  Completed 200 OK in 45ms (Views: 30.2ms | ActiveRecord: 12.1ms)
  ```
- **Proposed Format String**: `rails_log`
- **Use in Duck Hunt**:
  ```sql
  SELECT request_method, path, controller, action, status_code, duration_ms
  FROM read_duck_hunt_log('production.log', 'rails_log')
  WHERE status_code >= 400;
  ```

#### **29. R Logger**
- **Use Case**: R's logger package output.
- **Sample Format**:
  ```
  INFO [2025-01-15 10:30:45] Processing dataset
  ERROR [2025-01-15 10:30:46] Failed to load file: data.csv
  ```
- **Proposed Format String**: `r_logger`
- **Use in Duck Hunt**:
  ```sql
  SELECT severity, timestamp, message
  FROM read_duck_hunt_log('analysis.log', 'r_logger')
  WHERE severity = 'ERROR';
  ```

#### **30. Futile.logger (R)**
- **Use Case**: Popular R logging package with hierarchical loggers.
- **Sample Format**:
  ```
  INFO [2025-01-15 10:30:45] mypackage.module - Analysis complete
  ERROR [2025-01-15 10:30:46] mypackage.io - File not found
  ```
- **Proposed Format String**: `r_futile_logger`
- **Use in Duck Hunt**:
  ```sql
  SELECT severity, timestamp, logger, message
  FROM read_duck_hunt_log('analysis.log', 'r_futile_logger')
  WHERE severity = 'ERROR';
  ```

#### **31. Serilog (.NET)**
- **Use Case**: Popular structured logging for .NET applications.
- **Sample Format (JSON)**:
  ```json
  {"@t":"2025-01-15T10:30:45.123Z","@mt":"User {UserId} logged in","UserId":123,"@l":"Information"}
  ```
- **Sample Format (Text)**:
  ```
  [10:30:45 INF] User 123 logged in
  [10:30:46 ERR] Database connection failed
  ```
- **Proposed Format Strings**: `serilog_json`, `serilog_text`
- **Use in Duck Hunt**:
  ```sql
  SELECT level, message_template, timestamp
  FROM read_duck_hunt_log('app.log', 'serilog_json')
  WHERE level = 'Error';
  ```

#### **32. NLog (.NET)**
- **Use Case**: Flexible logging for .NET applications.
- **Sample Format**:
  ```
  2025-01-15 10:30:45.1234|INFO|MyApp.Program|Application started
  2025-01-15 10:30:46.5678|ERROR|MyApp.Service|Connection failed|System.TimeoutException
  ```
- **Proposed Format String**: `nlog_text`
- **Use in Duck Hunt**:
  ```sql
  SELECT timestamp, severity, logger, message, exception_type
  FROM read_duck_hunt_log('app.log', 'nlog_text')
  WHERE severity = 'ERROR';
  ```

---

### Cross-Language Structured Formats

These formats are used across multiple languages and ecosystems for structured logging.

#### **33. JSON Lines (JSONL/NDJSON)**
- **Use Case**: One JSON object per line - extremely common for structured logs.
- **Sample Format**:
  ```
  {"timestamp":"2025-01-15T10:30:45Z","level":"info","message":"Request received","request_id":"abc123"}
  {"timestamp":"2025-01-15T10:30:46Z","level":"error","message":"Request failed","request_id":"abc123","error":"timeout"}
  ```
- **Proposed Format String**: `jsonl`
- **Use in Duck Hunt**:
  ```sql
  SELECT level, message, request_id, timestamp
  FROM read_duck_hunt_log('app.log', 'jsonl')
  WHERE level = 'error';
  ```

#### **34. Logfmt**
- **Use Case**: Key=value pairs, popular in Go ecosystem and Heroku.
- **Sample Format**:
  ```
  level=info ts=2025-01-15T10:30:45Z msg="request completed" method=GET path=/api/users status=200 duration=45ms
  level=error ts=2025-01-15T10:30:46Z msg="database error" err="connection timeout"
  ```
- **Proposed Format String**: `logfmt`
- **Use in Duck Hunt**:
  ```sql
  SELECT level, msg, method, path, status, duration
  FROM read_duck_hunt_log('app.log', 'logfmt')
  WHERE level = 'error';
  ```

#### **35. GELF (Graylog Extended Log Format)**
- **Use Case**: Structured logging format designed for Graylog but widely adopted.
- **Sample Format**:
  ```json
  {"version":"1.1","host":"server1","short_message":"Request failed","level":3,"timestamp":1705315845.123,"_request_id":"abc123"}
  ```
- **Proposed Format String**: `gelf`
- **Use in Duck Hunt**:
  ```sql
  SELECT host, short_message, level, timestamp
  FROM read_duck_hunt_log('app.log', 'gelf')
  WHERE level <= 3;
  ```

#### **36. CEF (Common Event Format)**
- **Use Case**: Security logging standard used by SIEM tools.
- **Sample Format**:
  ```
  CEF:0|Security|ThreatManager|1.0|100|Malware detected|10|src=192.168.1.5 dst=10.0.0.1 spt=1234
  ```
- **Proposed Format String**: `cef`
- **Use in Duck Hunt**:
  ```sql
  SELECT vendor, product, event_name, severity, src_ip, dst_ip
  FROM read_duck_hunt_log('security.log', 'cef')
  WHERE severity >= 7;
  ```

---

### Log Aggregator Formats

These formats are specific to popular log aggregation and analysis platforms.

#### **37. Splunk JSON**
- **Use Case**: Logs formatted for Splunk ingestion.
- **Sample Format**:
  ```json
  {"time":1705315845,"event":{"level":"error","message":"Connection failed"},"source":"myapp","sourcetype":"application"}
  ```
- **Proposed Format String**: `splunk_json`
- **Use in Duck Hunt**:
  ```sql
  SELECT source, sourcetype, event_level, event_message, time
  FROM read_duck_hunt_log('splunk.log', 'splunk_json')
  WHERE event_level = 'error';
  ```

#### **38. Datadog JSON**
- **Use Case**: Logs formatted for Datadog ingestion.
- **Sample Format**:
  ```json
  {"message":"Request processed","status":"info","service":"api","ddsource":"nodejs","ddtags":"env:prod,version:1.2.3"}
  ```
- **Proposed Format String**: `datadog_json`
- **Use in Duck Hunt**:
  ```sql
  SELECT service, status, message, ddsource, ddtags
  FROM read_duck_hunt_log('datadog.log', 'datadog_json')
  WHERE status = 'error';
  ```

#### **39. Fluentd/Fluent Bit**
- **Use Case**: Logs from Fluentd/Fluent Bit log processors.
- **Sample Format**:
  ```json
  {"time":"2025-01-15T10:30:45+00:00","tag":"app.backend","record":{"level":"error","message":"timeout"}}
  ```
- **Proposed Format String**: `fluentd_json`
- **Use in Duck Hunt**:
  ```sql
  SELECT tag, record_level, record_message, time
  FROM read_duck_hunt_log('fluent.log', 'fluentd_json')
  WHERE record_level = 'error';
  ```

---

### Summary of Application Logging Formats

| Language/Platform | Format Strings | Priority |
|-------------------|----------------|----------|
| Python | `python_logging`, `structlog_json`, `loguru_text` | High |
| Node.js | `winston_json`, `winston_text`, `pino_json`, `bunyan_json` | High |
| Java/JVM | `log4j_text`, `java_stacktrace` | High |
| Go | `zap_json`, `logrus_json`, `logrus_text`, `zerolog_json` | Medium |
| Ruby | `ruby_logger`, `rails_log` | Medium |
| R | `r_logger`, `r_futile_logger` | Low |
| .NET | `serilog_json`, `serilog_text`, `nlog_text` | Medium |
| Cross-language | `jsonl`, `logfmt`, `gelf`, `cef` | High |
| Aggregators | `splunk_json`, `datadog_json`, `fluentd_json` | Medium |

**Implementation Priority:**
1. `jsonl` / `logfmt` - Universal structured formats
2. `python_logging`, `loguru_text` - Python ecosystem
3. `winston_json`, `pino_json` - Node.js ecosystem
4. `log4j_text`, `java_stacktrace` - Java ecosystem
5. `zap_json`, `logrus_json` - Go ecosystem
6. Platform-specific formats (Ruby, R, .NET)

---

## Schema Review Notes

### Phase 4 Schema Additions (IMPLEMENTED)

We added three new columns to support log formats beyond test results:

| Column | Type | Description |
|--------|------|-------------|
| `started_at` | VARCHAR | Event timestamp in original format |
| `principal` | VARCHAR | User/service identity (ARN, email, service account, authenticated user) |
| `origin` | VARCHAR | Network/system origin (IP address, hostname, runner name) |

These columns were chosen for semantic flexibility across format categories:
- **`origin`** was chosen over `ip_address` or `source` to handle both IP addresses and hostnames without implying a specific data type, and to avoid confusion with "source code"
- **`principal`** captures the "who" (identity/actor) across different authentication models
- **`started_at`** exposes timestamps without confusing the `function_name` column

### Current Field Mappings by Format Category

#### Cloud Audit Logs (AWS CloudTrail, GCP Cloud Logging, Azure Activity)

| Log Field | Column | Notes |
|-----------|--------|-------|
| Event timestamp | `started_at` | ✓ Proper semantic fit |
| API/Operation name | `function_name` | ✓ eventName, methodName, operationName |
| Service/Category | `category` | ✓ Service source (ec2.amazonaws.com, etc.) |
| Event description | `message` | ✓ Same as function_name for display |
| Error code | `error_code` | ✓ AWS error codes, status values |
| User identity | `principal` | ✓ ARN, email, service principal |
| Source IP | `origin` | ✓ Caller IP address |
| Region, Account ID | `structured_data` | JSON for cloud-specific metadata |

#### Web Access Logs (Apache, NGINX)

| Log Field | Column | Notes |
|-----------|--------|-------|
| Request timestamp | `started_at` | ✓ Proper semantic fit |
| Request path | `file_path` | ✓ The "file" being accessed |
| HTTP method | `category` | ✓ GET, POST, etc. as category |
| Status code | `error_code` | ✓ HTTP status (200, 404, 500) |
| Summary | `message` | ✓ "GET /path" for display |
| Client IP | `origin` | ✓ Request source |
| Request time (nginx) | `execution_time` | ✓ Request duration if available |
| Protocol, referrer, UA | `structured_data` | JSON for HTTP metadata |

#### System Logs (Syslog - BSD and RFC 5424)

| Log Field | Column | Notes |
|-----------|--------|-------|
| Log timestamp | `started_at` | ✓ Proper semantic fit |
| Process/Service | `category` | ✓ sshd, nginx, kernel |
| Log message | `message` | ✓ The log content |
| Hostname | `origin` | ✓ System that generated the log |
| Severity | `severity` | ✓ Mapped from RFC 5424 priority |
| PID, facility | `structured_data` | JSON for syslog metadata |

#### Build/Test/Lint Formats

For development-focused formats, `origin` and `principal` are typically empty since those concepts don't naturally apply. They could potentially be used for:
- `origin`: Build host, CI runner name, or build step name
- `principal`: Less applicable, but could be commit author

These mappings would need further discussion before implementation.

### Columns with Good Semantic Fit Across All Formats

| Column | Works Well For |
|--------|----------------|
| `category` | Service name, HTTP method, process name, linter name |
| `message` | Human-readable summary, log message, error description |
| `error_code` | HTTP status, AWS error codes, exit codes |
| `severity` | error/warning/info across all formats |
| `structured_data` | Format-specific metadata as JSON |
| `raw_output` | Original log line for reference |

### Infrastructure Log Formats (IMPLEMENTED)

#### Firewall Logs (iptables, pf, cisco_asa)

| Log Field | Column | Notes |
|-----------|--------|-------|
| Timestamp | `started_at` | Syslog timestamp |
| Source IP | `origin` | ✓ Network origin of traffic |
| Destination IP | `structured_data` | Only in JSON, no dedicated column |
| Action (block/allow) | `message` | Summary like "block: src -> dst (TCP)" |
| Hostname/Interface | `category` | Varies: hostname (iptables), interface (pf) |
| Severity level (ASA) | `error_code` | ASA message codes like "ASA-6-302013" |
| All fields | `structured_data` | Full JSON with src, dst, proto, ports, etc. |

**Awkwardness**: Destination IP has no dedicated column. For queries filtering by destination, must use `structured_data::json->>'dst'`.

#### VPC Flow Logs (AWS/GCP/Azure)

| Log Field | Column | Notes |
|-----------|--------|-------|
| Start timestamp | `started_at` | Converted from Unix epoch |
| Source IP | `origin` | ✓ Source address |
| Destination IP | `structured_data` | Only in JSON |
| Account ID | `principal` | ⚠️ AWS account, not exactly a "user identity" |
| Interface ID | `category` | ENI identifier |
| Action | `message` | "ACCEPT: src -> dst (TCP)" |
| Packets/bytes | `structured_data` | Traffic volume metrics |

**Awkwardness**: `principal` holds account_id which is an AWS account identifier, not a user/service identity. True principal would require CloudTrail correlation.

#### Kubernetes Logs (klog, kubectl)

| Log Field | Column | Notes |
|-----------|--------|-------|
| Timestamp | `started_at` | ✓ Log timestamp |
| Source file | `file_path` | ✓ Go source file (klog format) |
| Line number | `line_number` | ✓ Source line (klog format) |
| Log level | `severity` | Mapped from I/W/E/D prefixes |
| Context | `category` | File (klog), stream (kubectl), object (events) |
| Message | `message` | ✓ Log message content |

**Note**: Kubernetes logs come in multiple sub-formats (klog, kubectl logs, events). The parser handles all three but field availability varies.

#### Windows Event Logs

| Log Field | Column | Notes |
|-----------|--------|-------|
| Date | `started_at` | ✓ Event timestamp |
| User | `principal` | ✓ DOMAIN\username format |
| Event ID | `error_code` | ✓ Windows Event ID (4624, 4625, etc.) |
| Log Name/Source | `category` | Combined as "LogName/Source" |
| Level | `severity` | Mapped from Information/Warning/Error |
| Description | `message` | ✓ Event description text |
| All fields | `structured_data` | JSON with keywords, computer, etc. |

**Note**: Parser handles Windows Event Viewer text export format. XML format would require additional parsing.

#### Auditd / SSH Auth Logs

| Log Field | Column | Notes |
|-----------|--------|-------|
| Timestamp | `started_at` | ✓ Converted from audit epoch or syslog |
| Source IP | `origin` | ✓ SSH: client IP address |
| User | `principal` | SSH: username, Auditd: uid/auid info |
| Audit type | `category` | SYSCALL, USER_AUTH, sshd, etc. |
| Executable | `file_path` | ✓ Auditd: exe path |
| Event type | `error_code` | Audit type (also in category) |
| All fields | `structured_data` | Full audit key=value pairs as JSON |

**Note**: Auditd parser also handles SSH auth log format since they often appear together in security analysis.

#### S3 Access Logs

| Log Field | Column | Notes |
|-----------|--------|-------|
| Timestamp | `started_at` | ✓ Request timestamp |
| Remote IP | `origin` | ✓ Client IP address |
| Requester | `principal` | ✓ ARN or "-" for anonymous |
| Bucket | `category` | ✓ S3 bucket name |
| HTTP status | `error_code` | ✓ S3 error code (NoSuchKey, etc.) |
| Operation | `message` | "REST.GET.OBJECT key -> 200" |
| All fields | `structured_data` | request_id, bytes_sent, etc. |

### Schema Observations and Potential Improvements

#### What Works Well

1. **`origin`** - Consistently holds source IP/hostname across all infrastructure formats
2. **`started_at`** - Proper timestamp field now used by all log formats
3. **`structured_data`** - Essential escape hatch for format-specific fields
4. **`severity`** - Universal error/warning/info mapping works across all formats
5. **`error_code`** - Flexible enough for HTTP status, Event IDs, AWS errors, etc.

#### Current Limitations

1. **No destination IP column** - For network logs (firewall, VPC flow), destination IP must be extracted from `structured_data`. Consider adding `destination` column in future.

2. **`category` is overloaded** - Used for:
   - Service name (cloud audit)
   - HTTP method (web access)
   - Process name (syslog)
   - Interface (firewall)
   - Bucket name (S3)

   Works but semantically imprecise. Could consider `context` as a rename.

3. **`principal` semantic stretch** - For VPC Flow, holds AWS account_id which is technically correct but not a user identity. For auditd, holds uid strings like "auid=1000 uid=0" rather than a clean username.

4. **No protocol/port columns** - Network analysis often filters by protocol (TCP/UDP) or port. Currently requires JSON extraction.

5. **`function_name` unused** - Originally for test framework function names, now rarely populated for log formats. Could be repurposed or deprecated.

#### Future Considerations

For a v2 schema revision, consider:
- Dedicated network columns: `src_ip`, `dst_ip`, `src_port`, `dst_port`, `protocol`
- Rename `category` to `context` for clarity
- Add `actor` as alternative to `principal` for clearer semantics
- Consider whether `function_name` should be repurposed or removed

### Phase 3C Workflow Fields (Not Exposed)

The following fields exist in `ValidationEvent` but are not currently exposed in the table function schema:
- `workflow_name`, `job_name`, `step_name` - CI workflow hierarchy
- `workflow_run_id`, `job_id` - CI workflow identifiers
- `completed_at` - End timestamp (start is now `started_at`)

These remain internal for now but could be exposed if needed for CI log analysis.
